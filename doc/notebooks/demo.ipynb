{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Arrow shared between Python and R with rpy2\n",
    "\n",
    "The Python package shown here is available at:\n",
    "https://github.com/rpy2/rpy2-arrow\n",
    "\n",
    "**Warning:** This currently requires a nightly build of the R package `arrow`. It\n",
    "can be installed with the following command in R:\n",
    "\n",
    "```r\n",
    "install.packages(\"arrow\", repos = \"https://arrow-r-nightly.s3.amazonaws.com\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow\n",
    "import pyarrow.dataset as ds\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset in the NYC taxi one made available to download in the Parquet format by Ursa Labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'nyc-taxi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most will not have that data already available locally in directories. We use a Python\n",
    "translation of the code as https://ursalabs.org/arrow-r-nightly/articles/dataset.html\n",
    "to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     |            |\n",
      "2009 sssss\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This allows download an incomplete dataset\n",
    "# in the interest of time. Set it to None or -1\n",
    "# to download the complete dataset.\n",
    "MAX_NMONTHS = 5\n",
    "\n",
    "import os\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "import shutil\n",
    "\n",
    "BUCKET = 'https://ursa-labs-taxi-data.s3.us-east-2.amazonaws.com'\n",
    "paths = []\n",
    "print('     |            |')\n",
    "for year in range(2009, 2020):\n",
    "    if len(paths) == MAX_NMONTHS:\n",
    "        print()\n",
    "        break\n",
    "    print(f'{year} ', end='', flush=True)\n",
    "    if year == 2019:\n",
    "        # We only have through June 2019 there\n",
    "        months = range(1, 7)\n",
    "    else:\n",
    "        months = range(1, 13)\n",
    "    for month in months:\n",
    "        if len(paths) == MAX_NMONTHS:\n",
    "            print()\n",
    "            break\n",
    "        month_str = f'{month:02d}'\n",
    "        year_str = str(year)\n",
    "        url = urllib.parse.urljoin(BUCKET, '/'.join((year_str, month_str, 'data.parquet')))\n",
    "        filename = os.path.join(DATA_PATH, year_str, month_str, 'data.parquet')\n",
    "        if os.path.exists(filename):\n",
    "            print('s', end='', flush=True)\n",
    "            paths.append(filename)\n",
    "            continue\n",
    "        print('D', end='', flush=True)\n",
    "        os.makedirs(os.path.join(DATA_PATH, year_str, month_str))\n",
    "        with urllib.request.urlopen(url) as response, open(filename, 'wb') as output_file:\n",
    "            shutil.copyfileobj(response, output_file)\n",
    "        paths.append(filename)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is read using `pyarrow.dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._dataset.FileSystemDataset at 0x7ff5592355b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = ds.dataset(paths, format='parquet')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line reads the dataset into an Arrow table. Depending on the size\n",
    "of the data this may require a lot of memory so we apply a filter (check the relevant documentation\n",
    "for more details: https://arrow.apache.org/docs/python/dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120971, 18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbl = dataset.to_table(filter=ds.field('tip_amount') > 10)\n",
    "tbl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our Arrow table read with Python accessible to R by simply passing the R pointer we can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_tbl = pyra.converter.py2rpy(tbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table can then used in R, allowing to take advantage of individual strengths in a data science team\n",
    "(some prefer writing R code) or libraries in R for which there is arguably no matching equivalent in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: dplyr\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 19 x 2\u001b[39m\n",
      "   tip_group     n\n",
      "       \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m        10 \u001b[4m6\u001b[24m\u001b[4m6\u001b[24m301\n",
      "\u001b[90m 2\u001b[39m        15 \u001b[4m3\u001b[24m\u001b[4m3\u001b[24m250\n",
      "\u001b[90m 3\u001b[39m        20 \u001b[4m1\u001b[24m\u001b[4m0\u001b[24m285\n",
      "\u001b[90m 4\u001b[39m        25  \u001b[4m3\u001b[24m596\n",
      "\u001b[90m 5\u001b[39m        30  \u001b[4m2\u001b[24m328\n",
      "\u001b[90m 6\u001b[39m        35  \u001b[4m1\u001b[24m298\n",
      "\u001b[90m 7\u001b[39m        40   802\n",
      "\u001b[90m 8\u001b[39m        45   540\n",
      "\u001b[90m 9\u001b[39m        50   748\n",
      "\u001b[90m10\u001b[39m        55   551\n",
      "\u001b[90m11\u001b[39m        60   381\n",
      "\u001b[90m12\u001b[39m        65   181\n",
      "\u001b[90m13\u001b[39m        70   145\n",
      "\u001b[90m14\u001b[39m        75   132\n",
      "\u001b[90m15\u001b[39m        80   135\n",
      "\u001b[90m16\u001b[39m        85    69\n",
      "\u001b[90m17\u001b[39m        90    94\n",
      "\u001b[90m18\u001b[39m        95    55\n",
      "\u001b[90m19\u001b[39m       100    80\n"
     ]
    }
   ],
   "source": [
    "%%R -i r_tbl\n",
    "\n",
    "require(dplyr)\n",
    "r_tbl %>%\n",
    "  mutate(tip_group = round(tip_amount / 5) * 5) %>%\n",
    "  count(tip_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
